version: '3.7'

############################
## MINIO DEFAULT
x-minio-common: &minio-common
  image: quay.io/minio/minio:RELEASE.2023-12-07T04-16-00Z
  command: server --console-address ":9001" http://minio{1...3}/data{1...2}
  expose:
    - "9000"
    - "9001"
  environment:
    MINIO_ROOT_USER: admin
    MINIO_ROOT_PASSWORD: minioadmin
  healthcheck:
    test: ["CMD", "mc", "ready", "local"]
    interval: 5s
    timeout: 5s
    retries: 5

############################
## AIRFLOW DEFAULT
x-airflow-common:
  &airflow-common
  image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:2.7.3}
  environment:
    &airflow-common-env
    AIRFLOW__CORE__EXECUTOR: CeleryExecutor
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://admin:admin@db/airflow
    AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://admin:admin@db/airflow
    AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://admin:admin@db/airflow
    AIRFLOW__CELERY__BROKER_URL: redis://:@redis:6379/0
    AIRFLOW__CORE__FERNET_KEY: ''
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
    AIRFLOW__CORE__LOAD_EXAMPLES: 'true'
    AIRFLOW__API__AUTH_BACKENDS: 'airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session'
    AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: 'true'
    _PIP_ADDITIONAL_REQUIREMENTS: ${_PIP_ADDITIONAL_REQUIREMENTS:-}
  volumes:
    - ./airflow/dags:/opt/airflow/dags
    - ./airflow/logs:/opt/airflow/logs
    - ./airflow/config:/opt/airflow/config
    - ./airflow/plugins:/opt/airflow/plugins
    - ./util:/util
  user: "${AIRFLOW_UID:-50000}:0"
 

services:
##################################################
### MINIO
  minio1:
    <<: *minio-common
    hostname: minio1
    container_name: minio1
    volumes:
      - ./minio/data1-1:/data1
      - ./minio/data1-2:/data2
      - ./util:/util

  minio2:
    <<: *minio-common
    hostname: minio2
    container_name: minio2
    volumes:
      - ./minio/data2-1:/data1
      - ./minio/data2-2:/data2
      - ./util:/util

  minio3:
    <<: *minio-common
    hostname: minio3
    container_name: minio3
    volumes:
      - ./minio/data3-1:/data1
      - ./minio/data3-2:/data2
      - ./util:/util

  minio:
    image: nginx:1.19.2-alpine
    container_name: minio
    hostname: minio
    volumes:
      - ./minio/nginx.conf:/etc/nginx/nginx.conf:ro
    ports:
      - "9000:9000"
      - "9001:9001"
    depends_on:
      - minio1
      - minio2
      - minio3

##################################################
### KAFKA
  zookeeper:
    image: fjardim/mds-kafka-zookeeper
    hostname: zookeeper
    container_name: zookeeper
    ports:
       - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000

  kafka-broker:
    image: fjardim/mds-kafka-broker
    container_name: kafka-broker
    hostname: kafka-broker
    ports:
      - "9092:9092"
    depends_on:
      - zookeeper
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka-broker:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      CONFLUENT_METRICS_REPORTER_BOOTSTRAP_SERVERS: kafka-broker:9092
      CONFLUENT_METRICS_REPORTER_TOPIC_REPLICAS: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_CONFLUENT_LICENSE_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_CONFLUENT_BALANCER_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_JMX_PORT: 9101
      KAFKA_JMX_HOSTNAME: kafka-broker
      CONFLUENT_METRICS_ENABLE: 'true'
      CONFLUENT_SUPPORT_CUSTOMER_ID: 'anonymous'
      # AUTO_REGISTER_SCHEMAS : 'false'

  kafka-schema-registry:
    image: fjardim/mds-kafka-schema-registry
    hostname: kafka-schema-registry
    container_name: kafka-schema-registry
    depends_on:
      - kafka-broker
    ports:
      - "8071:8081"
    environment:
      SCHEMA_REGISTRY_HOST_NAME: schema-registry
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: 'kafka-broker:9092'
      SCHEMA_REGISTRY_LISTENERS: http://0.0.0.0:8081

  kafka-connect:
    image: fjardim/mds-kafka-connect
    hostname: kafka-connect
    container_name: kafka-connect
    depends_on:
      - kafka-broker
      - kafka-schema-registry
    ports:
      - "8083:8083"
    environment:
      CONNECT_BOOTSTRAP_SERVERS: 'kafka-broker:9092'
      CONNECT_REST_ADVERTISED_HOST_NAME: connect
      CONNECT_REST_PORT: 8083
      CONNECT_GROUP_ID: connect-group
      CONNECT_CONFIG_STORAGE_TOPIC: connect-configs
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_OFFSET_FLUSH_INTERVAL_MS: 10000
      CONNECT_OFFSET_STORAGE_TOPIC: connect-offsets
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_STATUS_STORAGE_TOPIC: connect-status
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_KEY_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_INTERNAL_KEY_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_INTERNAL_VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: http://kafka-schema-registry:8081
      # CLASSPATH required due to CC-2422
      CLASSPATH: /usr/share/java/monitoring-interceptors/monitoring-interceptors-6.1.0.jar
      CONNECT_PRODUCER_INTERCEPTOR_CLASSES: "io.confluent.monitoring.clients.interceptor.MonitoringProducerInterceptor"
      CONNECT_CONSUMER_INTERCEPTOR_CLASSES: "io.confluent.monitoring.clients.interceptor.MonitoringConsumerInterceptor"
      CONNECT_PLUGIN_PATH: "/usr/share/java,/usr/share/confluent-hub-components"
      CONNECT_LOG4J_LOGGERS: org.apache.zookeeper=ERROR,org.I0Itec.zkclient=ERROR,org.reflections=ERROR
      #CONNECT_REST_ADVERTISED_HOST_NAME: localhost
  
         
  kafka-control-center:
    image: fjardim/mds-kafka-control-center
    hostname: kafka-control-center
    container_name: kafka-control-center
    depends_on:
      - kafka-broker
      - kafka-schema-registry
      - kafka-connect
      - kafka-ksqldb-server
      - kafka-ksqldb-cli
    ports:
      - "9021:9021"
    environment:
      CONTROL_CENTER_BOOTSTRAP_SERVERS: 'kafka-broker:9092'
      CONTROL_CENTER_CONNECT_CLUSTER: 'kafka-connect:8083'
      CONTROL_CENTER_CONNECT_CONNECT-DEFAULT_CLUSTER: 'kafka-connect:8083'
      CONTROL_CENTER_KSQL_KSQLDB1_URL: "http://kafka-ksqldb-server:8088"
      CONTROL_CENTER_KSQL_KSQLDB1_ADVERTISED_URL: "http://kafka-ksqldb-server:8088"
      CONTROL_CENTER_SCHEMA_REGISTRY_URL: "http://kafka-schema-registry:8081"
      CONTROL_CENTER_REPLICATION_FACTOR: 1
      CONTROL_CENTER_INTERNAL_TOPICS_PARTITIONS: 1
      CONTROL_CENTER_MONITORING_INTERCEPTOR_TOPIC_PARTITIONS: 1
      CONFLUENT_METRICS_TOPIC_REPLICATION: 1
      CONTROL_CENTER_CONNECT_HEALTHCHECK_ENDPOINT: '/connectors'
      PORT: 9021

  kafka-ksqldb-server:
    image: fjardim/mds-kafka-ksqldb-server
    hostname: kafka-ksqldb-server
    container_name: kafka-ksqldb-server
    depends_on:
      - kafka-broker
      - kafka-connect
    ports:
      - "8068:8088"
    environment:
      KSQL_CONFIG_DIR: "/etc/ksql"
      KSQL_BOOTSTRAP_SERVERS: "kafka-broker:9092"
      KSQL_HOST_NAME: ksqldb-server
      KSQL_LISTENERS: "http://0.0.0.0:8088"
      KSQL_CACHE_MAX_BYTES_BUFFERING: 0
      KSQL_KSQL_SCHEMA_REGISTRY_URL: "http://kafka-schema-registry:8081"
      KSQL_PRODUCER_INTERCEPTOR_CLASSES: "io.confluent.monitoring.clients.interceptor.MonitoringProducerInterceptor"
      KSQL_CONSUMER_INTERCEPTOR_CLASSES: "io.confluent.monitoring.clients.interceptor.MonitoringConsumerInterceptor"
      KSQL_KSQL_CONNECT_URL: "http://kafka-connect:8083"
      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_REPLICATION_FACTOR: 1
      KSQL_KSQL_LOGGING_PROCESSING_TOPIC_AUTO_CREATE: 'true'
      KSQL_KSQL_LOGGING_PROCESSING_STREAM_AUTO_CREATE: 'true'

  kafka-ksqldb-cli:
    image: fjardim/mds-kafka-sqldb-cli
    container_name: kafka-ksqldb-cli
    hostname: kafka-ksqldb-cli
    depends_on:
      - kafka-broker
      - kafka-connect
      - kafka-ksqldb-server
    entrypoint: /bin/sh
    tty: true

  
  kafka-rest-proxy:
    image: fjardim/mds-kafka-rest
    hostname: kafka-rest-proxy
    depends_on:
      - kafka-broker
      - kafka-schema-registry
    ports:
      - 8082:8082
    #hostname: rest-proxy
    container_name: kafka-rest-proxy
    environment:
      KAFKA_REST_HOST_NAME: rest-proxy
      KAFKA_REST_BOOTSTRAP_SERVERS: 'kafka-broker:9092'
      KAFKA_REST_LISTENERS: "http://0.0.0.0:8082"
      KAFKA_REST_SCHEMA_REGISTRY_URL: 'http://schema-registry:8081'

##################################################
### HADOOP

  namenode:
    platform: linux/amd64
    #image: apache/hadoop:3
    image: fjardim/mds-namenode:3.3.6
    hostname: namenode
    container_name: namenode
    command: ["hdfs", "namenode"]
    ports:
      - 9870:9870
    env_file:
      - ./hadoop/config
    environment:
        ENSURE_NAMENODE_DIR: "/data/dfs/name"
    volumes:
        - ./hadoop/data/namenode:/data/dfs/name/
        - ./util:/util
    deploy:
      resources:
        limits:
          memory: 500m
          #cpus: '0.2'

  datanode:
    platform: linux/amd64
    #image: apache/hadoop:3
    image: fjardim/mds-datanode:3.3.6
    hostname: datanode
    container_name: datanode
    command: ["hdfs", "datanode"]
    environment:
      SERVICE_PRECONDITION: "namenode:9870"
      ENSURE_DATANODE_DIR:  "/data/dfs/data"
    env_file:
      - ./hadoop/config
    volumes:
      - ./hadoop/data/datanode:/data/dfs/data/
      - ./util:/util
    depends_on:
      - namenode
    deploy:
      resources:
        limits:
          memory: 500m
          #cpus: '0.2'

  resourcemanager:
    platform: linux/amd64
    #image: apache/hadoop:3
    image: fjardim/mds-resourcemanager:3.3.6
    hostname: resourcemanager
    container_name: resourcemanager
    command: ["yarn", "resourcemanager"]
    ports:
       - 28088:8088
       - 8030:8030
    env_file:
      - ./hadoop/config
    volumes:
      - ./util:/util
    deploy:
      resources:
        limits:
          memory: 500m
          #cpus: '0.2'

  nodemanager:
    platform: linux/amd64
    #image: apache/hadoop:3
    image: fjardim/mds-nodemanager:3.3.6
    hostname: nodemanager
    container_name: nodemanager
    command: ["yarn", "nodemanager"]
    env_file:
      - ./hadoop/config
    ports:
      - 8042:8042
    volumes:
      - ./util:/util
    deploy:
      resources:
        limits:
          memory: 500m
          #cpus: '3.0'

##################################################
### NIFI

  nifi:
      image: apache/nifi:latest
      container_name: nifi
      hostname: nifi
      volumes:
        - ./nifi/util:/util
        - ./nifi/database_repository:/opt/nifi/nifi-current/database_repository
        - ./nifi/flowfile_repository:/opt/nifi/nifi-current/flowfile_repository
        - ./nifi/content_repository:/opt/nifi/nifi-current/content_repository
        - ./nifi/provenance_repository:/opt/nifi/nifi-current/provenance_repository
        - ./nifi/state:/opt/nifi/nifi-current/state
        - ./nifi/conf:/opt/nifi/nifi-current/conf
        - ./util:/util
      environment:
        NIFI_WEB_HTTP_PORT: "9090"
        NIFI_WEB_HTTPS_HOST: "nifi"
      ports:
        - 49090:9090
      deploy:
        resources:
          limits:
            memory: 2g
            #cpus: '0.2'

  redis:
    image: redis:latest
    container_name: redis
    hostname: redis
    expose:
      - 6379
    ports:
        - 6379:6379
    volumes:
      - ./util:/util
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 30s
      retries: 50
      start_period: 30s
    restart: always      

##################################################
### PRESTO
  presto:
    platform: linux/amd64
    image: prestodb
    hostname: presto
    container_name: presto
    volumes: 
      - ./presto/etc/catalog:/opt/presto-server/etc/catalog/
      - ./presto/etc/catalog:/opt/presto/etc/catalog/
      - ./presto/etc/catalog:/etc/catalog/ 
      - ./presto/etc/hadoop:/hadoop 
      - ./util:/util
    ports:
      - 18080:8080
    deploy:
      resources:
        limits:
          memory: 2g
          #cpus: '0.2'

##################################################
### POSTGRESQL
  db:
    image: postgres
    container_name: db
    hostname: db
    environment:
      POSTGRES_PASSWORD: admin
      POSTGRES_USER: admin
      POSTGRES_DB: admin
    command: postgres -c shared_preload_libraries=pg_stat_statements -c pg_stat_statements.track=all -c max_connections=200 -c wal_level=logical
    ports:
      - 15432:5432
    volumes:
      - ./postgres/volume:/var/lib/postgresql/data
      - ./util:/util

  adminer:
    image: adminer
    container_name: adminer
    hostname: adminer
    ports:
      - 28080:8080

##################################################
### MONGO
  mongo:
    image: mongo
    container_name: mongo
    environment:
      MONGO_INITDB_ROOT_USERNAME: admin
      MONGO_INITDB_ROOT_PASSWORD: admin
    ports:
      - "27017:27017"
    volumes:
      - ./mongodb/data:/data/db
      - ./util:/util
    deploy:
      resources:
        limits:
          memory: 1g
          #cpus: '0.2'

  mongo-express:
    image: mongo-express
    container_name: mongo-express
    ports:
      - 28081:8081
    depends_on:
      - mongo
    environment:
      ME_CONFIG_MONGODB_ADMINUSERNAME: admin
      ME_CONFIG_MONGODB_ADMINPASSWORD: admin
      ME_CONFIG_MONGODB_URL: mongodb://root:root@mongo:27017/
      ME_CONFIG_BASICAUTH_USERNAME: admin
      ME_CONFIG_BASICAUTH_PASSWORD: admin

##################################################
### METABASE

  metabase:
      image: metabase/metabase:latest
      container_name: metabase
      hostname: metabase
      ports:
        - 3000:3000
      depends_on:
        - db
      environment:
        MB_DB_TYPE: postgres
        MB_DB_DBNAME: metabase
        MB_DB_PORT: 5432
        MB_DB_PASS: admin
        MB_DB_USER: admin
        MB_DB_HOST: db
        MB_PASSWORD_COMPLEXITY: "weak"
        MB_PASSWORD_LENGTH: 4
      volumes:
        - ./util:/util
      healthcheck:
        test: curl --fail -I http://localhost:3000/api/health || exit 1
        interval: 15s
        timeout: 5s
        retries: 5

##################################################
### HIVE
  hive:
    hostname: hive
    #image: apache/hive:4.0.0-beta-1
    image: fjardim/mds-hive
    container_name: hive
    environment:
      AWS_ACCESS_KEY_ID: datalake
      AWS_SECRET_ACCESS_KEY: datalake
      HIVE_CUSTOM_CONF_DIR: "/hive_custom_conf"
      SERVICE_NAME: hiveserver2
      SERVICE_OPTS: "-Dhive.metastore.uris=thrift://metastore:9083" 
      IS_RESUME: "true"
      #HIVE_VERSION: "3.1.3"
    ports:
       - "10000:10000"
       - "10002:10002"
    depends_on:
        - metastore
    user: root
    volumes:
       - ./hive/conf:/hive_custom_conf
       - ./util:/util
       
       
  metastore:
    hostname: metastore
    image: fjardim/mds-hive-metastore
    #image: apache/hive:4.0.0-beta-1
    container_name: metastore
    environment:
      AWS_ACCESS_KEY_ID: datalake
      AWS_SECRET_ACCESS_KEY: datalake
      HIVE_CUSTOM_CONF_DIR: "/hive_custom_conf"
      SERVICE_NAME: metastore
      #SERVICE_OPTS: "-Dhive.metastore.uris=thrift://metastore:9083" 
      IS_RESUME: "true"
      DB_DRIVER: postgres 
      SERVICE_OPTS: "-Djavax.jdo.option.ConnectionDriverName=org.postgresql.Driver -Djavax.jdo.option.ConnectionURL=jdbc:postgresql://db:5432/metastore -Djavax.jdo.option.ConnectionUserName=admin -Djavax.jdo.option.ConnectionPassword=admin" 
    ports:
       - "9083:9083"
    depends_on: 
      - db
    user: root
    volumes:
       - ./hive/meta:/opt/hive/data/warehouse 
       - ./hive/conf:/hive_custom_conf
       - ./util:/util
       
##################################################
### ELASTIC
  elasticsearch:
    image: elasticsearch:8.11.3
    container_name: elasticsearch
    hostname: elasticsearch
    ports:
      - "9200:9200"
      - "9300:9300"
    environment:
      discovery.type: "single-node"
      ES_JAVA_OPTS: "-Xms2g -Xmx2g"
      #ELASTIC_PASSWORD: "12345"
      xpack.security.enabled: "false"
    volumes:
      - ./elasticsearch/esdata:/usr/share/elasticsearch/data
      - ./util:/util
      
  kibana:
    image: kibana:8.11.3
    container_name: kibana
    hostname: kibana
    ports:
      - "5601:5601"
    environment:
      ELASTICSEARCH_URL: http://elasticsearch:9200
      #ELASTICSEARCH_USERNAME: "elastic"
      #ELASTICSEARCH_PASSWORD: "12345"
      #ELASTICSEARCH_SERVICEACCOUNTTOKEN: "GgXkNtUyT2CjF5k_488JcA"
    depends_on:
      - elasticsearch
##################################################
### SUPERSET
  superset:
    env_file: ./superset/docker/env-non-dev
    image: fjardim/mds-superset
    container_name: superset
    hostname: superset
    command: ["/app/docker/docker-bootstrap.sh", "app-gunicorn"]
    user: "root"
    ports:
      - 38088:8088
    depends_on: 
      - db
      #- redis
      - presto
    volumes: 
      - ./superset/docker:/app/docker
      - ./superset/superset_home:/app/superset_home
      - ./util:/util
      
##################################################
### SPARK
  spark-master:
    image: fjardim/mds-spark
    hostname: spark-master
    container_name: spark-master
    command: 
      - /bin/sh
      - -c
      - |
        /usr/local/spark/sbin/start-master.sh
        start-notebook.sh --NotebookApp.token=''
    ports:
      - 8889:8888
      - 4040:4040
      - 4041:4041
      - 4042:4042
      - 4043:4043
      - 38080:8080
      - 7077:7077
    volumes:
      - ./spark/work:/home/user 
      - ./spark/env:/env 
      - ./util:/util
    deploy:
      resources:
        limits:
          memory: 2g

  spark-worker:
    image: fjardim/mds-spark
    hostname: spark-worker
    container_name: spark-worker
    command: 
      - /bin/sh
      - -c
      - |
        /usr/local/spark/sbin/start-worker.sh spark-master:7077
        start-notebook.sh --NotebookApp.token='' 
    #command: /usr/local/spark/sbin/start-worker.sh jupyter-spark:7077
    #environment:
    #  PYSPARK_SUBMIT_ARGS: "--packages io.delta:delta-core_2.12:2.2.0,org.apache.hadoop:hadoop-aws:3.3.1,com.amazonaws:aws-java-sdk-bundle:1.12.392"
    env_file:
      - ./spark/env/jupyter.env
    ports:
      - 5040:4040
      - 5041:4041
      - 5042:4042
      - 5043:4043
      - 38081:8081
      - 36533:36533
    volumes:
      - ./util:/util
      - ./spark/work:/home/user 
    environment:
      SPARK_MASTER: spark-master
    depends_on:
        - spark-master
    deploy:
      resources:
        limits:
          memory: 1g

##################################################

  #execute-migrate-all:
    # container_name: execute_migrate_all
    # image: docker.getcollate.io/openmetadata/server:1.1.2
    # command: "./bootstrap/bootstrap_storage.sh migrate-all"
    # environment:
    #   OPENMETADATA_CLUSTER_NAME: ${OPENMETADATA_CLUSTER_NAME:-openmetadata}
    #   SERVER_PORT: ${SERVER_PORT:-8585}
    #   SERVER_ADMIN_PORT: ${SERVER_ADMIN_PORT:-8586}
    #   LOG_LEVEL: ${LOG_LEVEL:-INFO}
      
    #   # Migration 
    #   MIGRATION_LIMIT_PARAM: ${MIGRATION_LIMIT_PARAM:-1200}

    #   # OpenMetadata Server Authentication Configuration
    #   AUTHORIZER_CLASS_NAME: ${AUTHORIZER_CLASS_NAME:-org.openmetadata.service.security.DefaultAuthorizer}
    #   AUTHORIZER_REQUEST_FILTER: ${AUTHORIZER_REQUEST_FILTER:-org.openmetadata.service.security.JwtFilter}
    #   AUTHORIZER_ADMIN_PRINCIPALS: ${AUTHORIZER_ADMIN_PRINCIPALS:-[admin]}
    #   AUTHORIZER_ALLOWED_REGISTRATION_DOMAIN: ${AUTHORIZER_ALLOWED_REGISTRATION_DOMAIN:-["all"]}
    #   AUTHORIZER_INGESTION_PRINCIPALS: ${AUTHORIZER_INGESTION_PRINCIPALS:-[ingestion-bot]}
    #   AUTHORIZER_PRINCIPAL_DOMAIN: ${AUTHORIZER_PRINCIPAL_DOMAIN:-"openmetadata.org"}
    #   AUTHORIZER_ENFORCE_PRINCIPAL_DOMAIN: ${AUTHORIZER_ENFORCE_PRINCIPAL_DOMAIN:-false}
    #   AUTHORIZER_ENABLE_SECURE_SOCKET: ${AUTHORIZER_ENABLE_SECURE_SOCKET:-false}
    #   AUTHENTICATION_PROVIDER: ${AUTHENTICATION_PROVIDER:-basic}
    #   CUSTOM_OIDC_AUTHENTICATION_PROVIDER_NAME: ${CUSTOM_OIDC_AUTHENTICATION_PROVIDER_NAME:-""}
    #   AUTHENTICATION_PUBLIC_KEYS: ${AUTHENTICATION_PUBLIC_KEYS:-[http://localhost:8585/api/v1/system/config/jwks]}
    #   AUTHENTICATION_AUTHORITY: ${AUTHENTICATION_AUTHORITY:-https://accounts.google.com}
    #   AUTHENTICATION_CLIENT_ID: ${AUTHENTICATION_CLIENT_ID:-""}
    #   AUTHENTICATION_CALLBACK_URL: ${AUTHENTICATION_CALLBACK_URL:-""}
    #   AUTHENTICATION_JWT_PRINCIPAL_CLAIMS: ${AUTHENTICATION_JWT_PRINCIPAL_CLAIMS:-[email,preferred_username,sub]}
    #   AUTHENTICATION_ENABLE_SELF_SIGNUP: ${AUTHENTICATION_ENABLE_SELF_SIGNUP:-true}

    #   # JWT Configuration
    #   RSA_PUBLIC_KEY_FILE_PATH: ${RSA_PUBLIC_KEY_FILE_PATH:-"./conf/public_key.der"}
    #   RSA_PRIVATE_KEY_FILE_PATH: ${RSA_PRIVATE_KEY_FILE_PATH:-"./conf/private_key.der"}
    #   JWT_ISSUER: ${JWT_ISSUER:-"open-metadata.org"}
    #   JWT_KEY_ID: ${JWT_KEY_ID:-"Gb389a-9f76-gdjs-a92j-0242bk94356"}
    #   # OpenMetadata Server Pipeline Service Client Configuration
    #   PIPELINE_SERVICE_CLIENT_ENDPOINT: ${PIPELINE_SERVICE_CLIENT_ENDPOINT:-http://ingestion:8080}
    #   PIPELINE_SERVICE_CLIENT_HEALTH_CHECK_INTERVAL: ${PIPELINE_SERVICE_CLIENT_HEALTH_CHECK_INTERVAL:-300}
    #   SERVER_HOST_API_URL: ${SERVER_HOST_API_URL:-http://openmetadata-server:8585/api}
    #   PIPELINE_SERVICE_CLIENT_VERIFY_SSL: ${PIPELINE_SERVICE_CLIENT_VERIFY_SSL:-"no-ssl"}
    #   PIPELINE_SERVICE_CLIENT_SSL_CERT_PATH: ${PIPELINE_SERVICE_CLIENT_SSL_CERT_PATH:-""}
    #   # Database configuration for MySQL
    #   DB_DRIVER_CLASS: "org.postgresql.Driver"
    #   DB_SCHEME: "postgresql"
    #   DB_USE_SSL: "false"
    #   DB_USER: "admin"
    #   DB_USER_PASSWORD: "admin"
    #   DB_HOST: "db"
    #   DB_PORT: "5432"
    #   OM_DATABASE: "openmetadata"
    #   # ElasticSearch Configurations
    #   ELASTICSEARCH_HOST: ${ELASTICSEARCH_HOST:- elasticsearch}
    #   ELASTICSEARCH_PORT: ${ELASTICSEARCH_PORT:-9200}
    #   ELASTICSEARCH_SCHEME: ${ELASTICSEARCH_SCHEME:-http}
    #   ELASTICSEARCH_USER: ${ELASTICSEARCH_USER:-""}
    #   ELASTICSEARCH_PASSWORD: ${ELASTICSEARCH_PASSWORD:-""}
    #   SEARCH_TYPE: ${SEARCH_TYPE:- "elasticsearch"}
    #   ELASTICSEARCH_TRUST_STORE_PATH: ${ELASTICSEARCH_TRUST_STORE_PATH:-""}
    #   ELASTICSEARCH_TRUST_STORE_PASSWORD: ${ELASTICSEARCH_TRUST_STORE_PASSWORD:-""}
    #   ELASTICSEARCH_CONNECTION_TIMEOUT_SECS: ${ELASTICSEARCH_CONNECTION_TIMEOUT_SECS:-5}
    #   ELASTICSEARCH_SOCKET_TIMEOUT_SECS: ${ELASTICSEARCH_SOCKET_TIMEOUT_SECS:-60}
    #   ELASTICSEARCH_KEEP_ALIVE_TIMEOUT_SECS: ${ELASTICSEARCH_KEEP_ALIVE_TIMEOUT_SECS:-600}
    #   ELASTICSEARCH_BATCH_SIZE: ${ELASTICSEARCH_BATCH_SIZE:-10}
    #   ELASTICSEARCH_INDEX_MAPPING_LANG: ${ELASTICSEARCH_INDEX_MAPPING_LANG:-EN}

    #   #eventMonitoringConfiguration
    #   EVENT_MONITOR: ${EVENT_MONITOR:-prometheus}
    #   EVENT_MONITOR_BATCH_SIZE: ${EVENT_MONITOR_BATCH_SIZE:-10}
    #   EVENT_MONITOR_PATH_PATTERN: ${EVENT_MONITOR_PATH_PATTERN:-["/api/v1/tables/*", "/api/v1/health-check"]}
    #   EVENT_MONITOR_LATENCY: ${EVENT_MONITOR_LATENCY:-[]}

    #   #pipelineServiceClientConfiguration
    #   PIPELINE_SERVICE_CLIENT_CLASS_NAME: ${PIPELINE_SERVICE_CLIENT_CLASS_NAME:-"org.openmetadata.service.clients.pipeline.airflow.AirflowRESTClient"}
    #   PIPELINE_SERVICE_IP_INFO_ENABLED: ${PIPELINE_SERVICE_IP_INFO_ENABLED:-false}
    #   PIPELINE_SERVICE_CLIENT_HOST_IP: ${PIPELINE_SERVICE_CLIENT_HOST_IP:-""}
    #   PIPELINE_SERVICE_CLIENT_SECRETS_MANAGER_LOADER: ${PIPELINE_SERVICE_CLIENT_SECRETS_MANAGER_LOADER:-"noop"}
    #   #airflow parameters
    #   AIRFLOW_USERNAME: "airflow"
    #   AIRFLOW_PASSWORD: "airflow"
    #   AIRFLOW_TIMEOUT: ${AIRFLOW_TIMEOUT:-10}
    #   AIRFLOW_TRUST_STORE_PATH: ${AIRFLOW_TRUST_STORE_PATH:-""}
    #   AIRFLOW_TRUST_STORE_PASSWORD: ${AIRFLOW_TRUST_STORE_PASSWORD:-""}
    #   FERNET_KEY: ${FERNET_KEY:-jJ/9sz0g0OHxsfxOoSfdFdmk3ysNmPRnH3TUAbz3IHA=}

    #   #secretsManagerConfiguration
    #   SECRET_MANAGER: ${SECRET_MANAGER:-noop}
    #   #parameters:
    #   OM_SM_REGION: ${OM_SM_REGION:-""}
    #   OM_SM_ACCESS_KEY_ID: ${OM_SM_ACCESS_KEY_ID:-""}
    #   OM_SM_ACCESS_KEY: ${OM_SM_ACCESS_KEY:-""}

    #   #email configuration:
    #   OM_EMAIL_ENTITY: ${OM_EMAIL_ENTITY:-"OpenMetadata"}
    #   OM_SUPPORT_URL: ${OM_SUPPORT_URL:-"https://slack.open-metadata.org"}
    #   AUTHORIZER_ENABLE_SMTP : ${AUTHORIZER_ENABLE_SMTP:-false}
    #   OPENMETADATA_SERVER_URL: ${OPENMETADATA_SERVER_URL:-""}
    #   OPENMETADATA_SMTP_SENDER_MAIL: ${OPENMETADATA_SMTP_SENDER_MAIL:-""}
    #   SMTP_SERVER_ENDPOINT: ${SMTP_SERVER_ENDPOINT:-""}
    #   SMTP_SERVER_PORT: ${SMTP_SERVER_PORT:-""}
    #   SMTP_SERVER_USERNAME: ${SMTP_SERVER_USERNAME:-""}
    #   SMTP_SERVER_PWD: ${SMTP_SERVER_PWD:-""}
    #   SMTP_SERVER_STRATEGY: ${SMTP_SERVER_STRATEGY:-"SMTP_TLS"}

    #   #changeEventConfig
    #   OM_URI: ${OM_URI:- "http://localhost:8585"}

    #   #extensionConfiguration
    #   OM_RESOURCE_PACKAGES: ${OM_RESOURCE_PACKAGES:-[]}
    #   OM_EXTENSIONS: ${OM_EXTENSIONS:-[]}


    #   # Heap OPTS Configurations
    #   OPENMETADATA_HEAP_OPTS: ${OPENMETADATA_HEAP_OPTS:--Xmx1G -Xms1G}
    #   # Application Config
    #   OM_CUSTOM_LOGO_URL_PATH: ${OM_CUSTOM_LOGO_URL_PATH:-""}
    #   OM_CUSTOM_MONOGRAM_URL_PATH: ${OM_CUSTOM_MONOGRAM_URL_PATH:-""}
    #   OM_MAX_FAILED_LOGIN_ATTEMPTS: ${OM_MAX_FAILED_LOGIN_ATTEMPTS:-3}
    #   OM_LOGIN_ACCESS_BLOCK_TIME: ${OM_LOGIN_ACCESS_BLOCK_TIME:-600}
    #   OM_JWT_EXPIRY_TIME: ${OM_JWT_EXPIRY_TIME:-3600}
    #   # Mask passwords values in UI
    #   MASK_PASSWORDS_API: ${MASK_PASSWORDS_API:-false}

    #   #OpenMetadata Web Configuration
    #   WEB_CONF_URI_PATH: ${WEB_CONF_URI_PATH:-"/api"}
    #   #HSTS
    #   WEB_CONF_HSTS_ENABLED: ${WEB_CONF_HSTS_ENABLED:-false}
    #   WEB_CONF_HSTS_MAX_AGE: ${WEB_CONF_HSTS_MAX_AGE:-"365 days"}
    #   WEB_CONF_HSTS_INCLUDE_SUBDOMAINS: ${WEB_CONF_HSTS_INCLUDE_SUBDOMAINS:-"true"}
    #   WEB_CONF_HSTS_PRELOAD: ${WEB_CONF_HSTS_PRELOAD:-"true"}
    #   #Frame Options
    #   WEB_CONF_FRAME_OPTION_ENABLED: ${WEB_CONF_FRAME_OPTION_ENABLED:-false}
    #   WEB_CONF_FRAME_OPTION: ${WEB_CONF_FRAME_OPTION:-"SAMEORIGIN"}
    #   WEB_CONF_FRAME_ORIGIN: ${WEB_CONF_FRAME_ORIGIN:-""}
    #   #Content Type
    #   WEB_CONF_CONTENT_TYPE_OPTIONS_ENABLED: ${WEB_CONF_CONTENT_TYPE_OPTIONS_ENABLED:-false}
    #   #XSS-Protection  
    #   WEB_CONF_XSS_PROTECTION_ENABLED: ${WEB_CONF_XSS_PROTECTION_ENABLED:-false}
    #   WEB_CONF_XSS_PROTECTION_ON: ${WEB_CONF_XSS_PROTECTION_ON:-true}
    #   WEB_CONF_XSS_PROTECTION_BLOCK: ${WEB_CONF_XSS_PROTECTION_BLOCK:-true}
    #   #CSP    
    #   WEB_CONF_XSS_CSP_ENABLED: ${WEB_CONF_XSS_CSP_ENABLED:-false}
    #   WEB_CONF_XSS_CSP_POLICY: ${WEB_CONF_XSS_CSP_POLICY:-"default-src 'self'"}
    #   WEB_CONF_XSS_CSP_REPORT_ONLY_POLICY: ${WEB_CONF_XSS_CSP_REPORT_ONLY_POLICY:-""}
    #   #Referrer-Policy
    #   WEB_CONF_REFERRER_POLICY_ENABLED: ${WEB_CONF_REFERRER_POLICY_ENABLED:-false}
    #   WEB_CONF_REFERRER_POLICY_OPTION: ${WEB_CONF_REFERRER_POLICY_OPTION:-"SAME_ORIGIN"}
    #   #Permission-Policy
    #   WEB_CONF_PERMISSION_POLICY_ENABLED: ${WEB_CONF_PERMISSION_POLICY_ENABLED:-false}
    #   WEB_CONF_PERMISSION_POLICY_OPTION: ${WEB_CONF_PERMISSION_POLICY_OPTION:-""}
    


  openmetadata-server:
    container_name: openmetadata_server
    restart: always
    image: docker.getcollate.io/openmetadata/server:1.1.2
    environment:
      OPENMETADATA_CLUSTER_NAME: ${OPENMETADATA_CLUSTER_NAME:-openmetadata}
      SERVER_PORT: ${SERVER_PORT:-8585}
      SERVER_ADMIN_PORT: ${SERVER_ADMIN_PORT:-8586}
      LOG_LEVEL: ${LOG_LEVEL:-INFO}

      # OpenMetadata Server Authentication Configuration
      AUTHORIZER_CLASS_NAME: ${AUTHORIZER_CLASS_NAME:-org.openmetadata.service.security.DefaultAuthorizer}
      AUTHORIZER_REQUEST_FILTER: ${AUTHORIZER_REQUEST_FILTER:-org.openmetadata.service.security.JwtFilter}
      AUTHORIZER_ADMIN_PRINCIPALS: ${AUTHORIZER_ADMIN_PRINCIPALS:-[admin]}
      AUTHORIZER_ALLOWED_REGISTRATION_DOMAIN: ${AUTHORIZER_ALLOWED_REGISTRATION_DOMAIN:-["all"]}
      AUTHORIZER_INGESTION_PRINCIPALS: ${AUTHORIZER_INGESTION_PRINCIPALS:-[ingestion-bot]}
      AUTHORIZER_PRINCIPAL_DOMAIN: ${AUTHORIZER_PRINCIPAL_DOMAIN:-"openmetadata.org"}
      AUTHORIZER_ENFORCE_PRINCIPAL_DOMAIN: ${AUTHORIZER_ENFORCE_PRINCIPAL_DOMAIN:-false}
      AUTHORIZER_ENABLE_SECURE_SOCKET: ${AUTHORIZER_ENABLE_SECURE_SOCKET:-false}
      AUTHENTICATION_PROVIDER: ${AUTHENTICATION_PROVIDER:-basic}
      CUSTOM_OIDC_AUTHENTICATION_PROVIDER_NAME: ${CUSTOM_OIDC_AUTHENTICATION_PROVIDER_NAME:-""}
      AUTHENTICATION_PUBLIC_KEYS: ${AUTHENTICATION_PUBLIC_KEYS:-[http://localhost:8585/api/v1/system/config/jwks]}
      AUTHENTICATION_AUTHORITY: ${AUTHENTICATION_AUTHORITY:-https://accounts.google.com}
      AUTHENTICATION_CLIENT_ID: ${AUTHENTICATION_CLIENT_ID:-""}
      AUTHENTICATION_CALLBACK_URL: ${AUTHENTICATION_CALLBACK_URL:-""}
      AUTHENTICATION_JWT_PRINCIPAL_CLAIMS: ${AUTHENTICATION_JWT_PRINCIPAL_CLAIMS:-[email,preferred_username,sub]}
      AUTHENTICATION_ENABLE_SELF_SIGNUP: ${AUTHENTICATION_ENABLE_SELF_SIGNUP:-true}

      # JWT Configuration
      RSA_PUBLIC_KEY_FILE_PATH: ${RSA_PUBLIC_KEY_FILE_PATH:-"./conf/public_key.der"}
      RSA_PRIVATE_KEY_FILE_PATH: ${RSA_PRIVATE_KEY_FILE_PATH:-"./conf/private_key.der"}
      JWT_ISSUER: ${JWT_ISSUER:-"open-metadata.org"}
      JWT_KEY_ID: ${JWT_KEY_ID:-"Gb389a-9f76-gdjs-a92j-0242bk94356"}
      # OpenMetadata Server Pipeline Service Client Configuration
      PIPELINE_SERVICE_CLIENT_ENDPOINT: ${PIPELINE_SERVICE_CLIENT_ENDPOINT:-http://ingestion:8080}
      PIPELINE_SERVICE_CLIENT_HEALTH_CHECK_INTERVAL: ${PIPELINE_SERVICE_CLIENT_HEALTH_CHECK_INTERVAL:-300}
      SERVER_HOST_API_URL: ${SERVER_HOST_API_URL:-http://openmetadata-server:8585/api}
      PIPELINE_SERVICE_CLIENT_VERIFY_SSL: ${PIPELINE_SERVICE_CLIENT_VERIFY_SSL:-"no-ssl"}
      PIPELINE_SERVICE_CLIENT_SSL_CERT_PATH: ${PIPELINE_SERVICE_CLIENT_SSL_CERT_PATH:-""}
      # Database configuration for MySQL
      DB_DRIVER_CLASS: "org.postgresql.Driver"
      DB_SCHEME: "postgresql"
      DB_USE_SSL: "false"
      DB_USER: "admin"
      DB_USER_PASSWORD: "admin"
      DB_HOST: "db"
      DB_PORT: "5432"
      OM_DATABASE: "openmetadata"
      # ElasticSearch Configurations
      ELASTICSEARCH_HOST: ${ELASTICSEARCH_HOST:- elasticsearch}
      ELASTICSEARCH_PORT: ${ELASTICSEARCH_PORT:-9200}
      ELASTICSEARCH_SCHEME: ${ELASTICSEARCH_SCHEME:-http}
      ELASTICSEARCH_USER: ${ELASTICSEARCH_USER:-""}
      ELASTICSEARCH_PASSWORD: ${ELASTICSEARCH_PASSWORD:-""}
      SEARCH_TYPE: ${SEARCH_TYPE:- "elasticsearch"}
      ELASTICSEARCH_TRUST_STORE_PATH: ${ELASTICSEARCH_TRUST_STORE_PATH:-""}
      ELASTICSEARCH_TRUST_STORE_PASSWORD: ${ELASTICSEARCH_TRUST_STORE_PASSWORD:-""}
      ELASTICSEARCH_CONNECTION_TIMEOUT_SECS: ${ELASTICSEARCH_CONNECTION_TIMEOUT_SECS:-5}
      ELASTICSEARCH_SOCKET_TIMEOUT_SECS: ${ELASTICSEARCH_SOCKET_TIMEOUT_SECS:-60}
      ELASTICSEARCH_KEEP_ALIVE_TIMEOUT_SECS: ${ELASTICSEARCH_KEEP_ALIVE_TIMEOUT_SECS:-600}
      ELASTICSEARCH_BATCH_SIZE: ${ELASTICSEARCH_BATCH_SIZE:-10}
      ELASTICSEARCH_INDEX_MAPPING_LANG: ${ELASTICSEARCH_INDEX_MAPPING_LANG:-EN}

      #eventMonitoringConfiguration
      EVENT_MONITOR: ${EVENT_MONITOR:-prometheus}
      EVENT_MONITOR_BATCH_SIZE: ${EVENT_MONITOR_BATCH_SIZE:-10}
      EVENT_MONITOR_PATH_PATTERN: ${EVENT_MONITOR_PATH_PATTERN:-["/api/v1/tables/*", "/api/v1/health-check"]}
      EVENT_MONITOR_LATENCY: ${EVENT_MONITOR_LATENCY:-[]}

      #pipelineServiceClientConfiguration
      PIPELINE_SERVICE_CLIENT_CLASS_NAME: ${PIPELINE_SERVICE_CLIENT_CLASS_NAME:-"org.openmetadata.service.clients.pipeline.airflow.AirflowRESTClient"}
      PIPELINE_SERVICE_IP_INFO_ENABLED: ${PIPELINE_SERVICE_IP_INFO_ENABLED:-false}
      PIPELINE_SERVICE_CLIENT_HOST_IP: ${PIPELINE_SERVICE_CLIENT_HOST_IP:-""}
      PIPELINE_SERVICE_CLIENT_SECRETS_MANAGER_LOADER: ${PIPELINE_SERVICE_CLIENT_SECRETS_MANAGER_LOADER:-"noop"}
      #airflow parameters
      AIRFLOW_USERNAME: ${AIRFLOW_USERNAME:-admin}
      AIRFLOW_PASSWORD: ${AIRFLOW_PASSWORD:-admin}
      AIRFLOW_TIMEOUT: ${AIRFLOW_TIMEOUT:-10}
      AIRFLOW_TRUST_STORE_PATH: ${AIRFLOW_TRUST_STORE_PATH:-""}
      AIRFLOW_TRUST_STORE_PASSWORD: ${AIRFLOW_TRUST_STORE_PASSWORD:-""}
      FERNET_KEY: ${FERNET_KEY:-jJ/9sz0g0OHxsfxOoSfdFdmk3ysNmPRnH3TUAbz3IHA=}

      #secretsManagerConfiguration
      SECRET_MANAGER: ${SECRET_MANAGER:-noop}
      #parameters:
      OM_SM_REGION: ${OM_SM_REGION:-""}
      OM_SM_ACCESS_KEY_ID: ${OM_SM_ACCESS_KEY_ID:-""}
      OM_SM_ACCESS_KEY: ${OM_SM_ACCESS_KEY:-""}
      
      #email configuration:
      OM_EMAIL_ENTITY: ${OM_EMAIL_ENTITY:-"OpenMetadata"}
      OM_SUPPORT_URL: ${OM_SUPPORT_URL:-"https://slack.open-metadata.org"}
      AUTHORIZER_ENABLE_SMTP : ${AUTHORIZER_ENABLE_SMTP:-false}
      OPENMETADATA_SERVER_URL: ${OPENMETADATA_SERVER_URL:-""}
      OPENMETADATA_SMTP_SENDER_MAIL: ${OPENMETADATA_SMTP_SENDER_MAIL:-""}
      SMTP_SERVER_ENDPOINT: ${SMTP_SERVER_ENDPOINT:-""}
      SMTP_SERVER_PORT: ${SMTP_SERVER_PORT:-""}
      SMTP_SERVER_USERNAME: ${SMTP_SERVER_USERNAME:-""}
      SMTP_SERVER_PWD: ${SMTP_SERVER_PWD:-""}
      SMTP_SERVER_STRATEGY: ${SMTP_SERVER_STRATEGY:-"SMTP_TLS"}

      #changeEventConfig
      OM_URI: ${OM_URI:- "http://localhost:8585"}

      #extensionConfiguration
      OM_RESOURCE_PACKAGES: ${OM_RESOURCE_PACKAGES:-[]}
      OM_EXTENSIONS: ${OM_EXTENSIONS:-[]}


      # Heap OPTS Configurations
      OPENMETADATA_HEAP_OPTS: ${OPENMETADATA_HEAP_OPTS:--Xmx1G -Xms1G}
      # Application Config
      OM_CUSTOM_LOGO_URL_PATH: ${OM_CUSTOM_LOGO_URL_PATH:-""}
      OM_CUSTOM_MONOGRAM_URL_PATH: ${OM_CUSTOM_MONOGRAM_URL_PATH:-""}
      OM_MAX_FAILED_LOGIN_ATTEMPTS: ${OM_MAX_FAILED_LOGIN_ATTEMPTS:-3}
      OM_LOGIN_ACCESS_BLOCK_TIME: ${OM_LOGIN_ACCESS_BLOCK_TIME:-600}
      OM_JWT_EXPIRY_TIME: ${OM_JWT_EXPIRY_TIME:-3600}
      # Mask passwords values in UI
      MASK_PASSWORDS_API: ${MASK_PASSWORDS_API:-false}
      
      #OpenMetadata Web Configuration
      WEB_CONF_URI_PATH: ${WEB_CONF_URI_PATH:-"/api"}
      #HSTS
      WEB_CONF_HSTS_ENABLED: ${WEB_CONF_HSTS_ENABLED:-false}
      WEB_CONF_HSTS_MAX_AGE: ${WEB_CONF_HSTS_MAX_AGE:-"365 days"}
      WEB_CONF_HSTS_INCLUDE_SUBDOMAINS: ${WEB_CONF_HSTS_INCLUDE_SUBDOMAINS:-"true"}
      WEB_CONF_HSTS_PRELOAD: ${WEB_CONF_HSTS_PRELOAD:-"true"}
      #Frame Options
      WEB_CONF_FRAME_OPTION_ENABLED: ${WEB_CONF_FRAME_OPTION_ENABLED:-false}
      WEB_CONF_FRAME_OPTION: ${WEB_CONF_FRAME_OPTION:-"SAMEORIGIN"}
      WEB_CONF_FRAME_ORIGIN: ${WEB_CONF_FRAME_ORIGIN:-""}
      #Content Type
      WEB_CONF_CONTENT_TYPE_OPTIONS_ENABLED: ${WEB_CONF_CONTENT_TYPE_OPTIONS_ENABLED:-false}
      #XSS-Protection  
      WEB_CONF_XSS_PROTECTION_ENABLED: ${WEB_CONF_XSS_PROTECTION_ENABLED:-false}
      WEB_CONF_XSS_PROTECTION_ON: ${WEB_CONF_XSS_PROTECTION_ON:-true}
      WEB_CONF_XSS_PROTECTION_BLOCK: ${WEB_CONF_XSS_PROTECTION_BLOCK:-true}
      #CSP    
      WEB_CONF_XSS_CSP_ENABLED: ${WEB_CONF_XSS_CSP_ENABLED:-false}
      WEB_CONF_XSS_CSP_POLICY: ${WEB_CONF_XSS_CSP_POLICY:-"default-src 'self'"}
      WEB_CONF_XSS_CSP_REPORT_ONLY_POLICY: ${WEB_CONF_XSS_CSP_REPORT_ONLY_POLICY:-""}
      
    expose:
      - 8585
      - 8586
    ports:
      - "8585:8585"
      - "8586:8586"
    depends_on:
      - elasticsearch
      - db
    healthcheck:
      test: [ "CMD", "wget", "-q", "--spider",  "http://localhost:8586/healthcheck" ]
    volumes:
      - ./util:/util
  
  ingestion:
    container_name: openmetadata_ingestion
    image: docker.getcollate.io/openmetadata/ingestion:1.1.2
    depends_on:
      - openmetadata-server
    environment:
      AIRFLOW__API__AUTH_BACKENDS: "airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session"
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__OPENMETADATA_AIRFLOW_APIS__DAG_GENERATED_CONFIGS: "/opt/airflow/dag_generated_configs"
      DB_HOST: "db"
      DB_PORT: "5432"
      AIRFLOW_DB: "openmetadata_airflow"
      DB_SCHEME: "postgresql+psycopg2"
      DB_USER: "admin"
      DB_PASSWORD: "admin"
    entrypoint: /bin/bash
    command:
      - "/opt/airflow/ingestion_dependency.sh"
    ports:
      - "48080:8080"
    volumes:
      - ./openmetadata/dag-airflow:/opt/airflow/dag_generated_configs
      - ./openmetadata/dags:/opt/airflow/dags
      - ./openmetadata/mp:/tmp
      - ./util:/util

#############################
## AIRFLOW
  airflow-webserver:
    <<: *airflow-common
    container_name: airflow-webserver
    hostname: airflow-webserver
    command: webserver
    ports:
      - "58080:8080"
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    depends_on:
      - redis
      - db
      - airflow-scheduler
      - airflow-worker

  airflow-scheduler:
    <<: *airflow-common
    container_name: airflow-scheduler
    hostname: airflow-scheduler
    command: scheduler
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8974/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    depends_on:
      - redis
      - db

  airflow-worker:
    <<: *airflow-common
    container_name: airflow-worker
    hostname: airflow-worker
    command: celery worker
    healthcheck:
      # yamllint disable rule:line-length
      test:
        - "CMD-SHELL"
        - 'celery --app airflow.providers.celery.executors.celery_executor.app inspect ping -d "celery@$${HOSTNAME}" || celery --app airflow.executors.celery_executor.app inspect ping -d "celery@$${HOSTNAME}"'
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    environment:
      <<: *airflow-common-env
      # Required to handle warm shutdown of the celery workers properly
      # See https://airflow.apache.org/docs/docker-stack/entrypoint.html#signal-propagation
      DUMB_INIT_SETSID: "0"
    depends_on:
      - redis
      - db

  airflow-triggerer:
    <<: *airflow-common
    container_name: airflow-triggerer
    hostname: airflow-triggerer
    command: triggerer
    healthcheck:
      test: ["CMD-SHELL", 'airflow jobs check --job-type TriggererJob --hostname "$${HOSTNAME}"']
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    depends_on:
      - redis
      - db

  airflow-cli:
    <<: *airflow-common
    container_name: airflow-cli
    hostname: airflow-cli
    profiles:
      - debug
    environment:
      <<: *airflow-common-env
      CONNECTION_CHECK_MAX_COUNT: "0"
    # Workaround for entrypoint issue. See: https://github.com/apache/airflow/issues/16252
    command:
      - bash
      - -c
      - airflow

  # You can enable flower by adding "--profile flower" option e.g. docker-compose --profile flower up
  # or by explicitly targeted on the command line e.g. docker-compose up flower.
  # See: https://docs.docker.com/compose/profiles/
  airflow-flower:
    <<: *airflow-common
    container_name: airflow-flower
    hostname: airflow-flower
    command: celery flower
    profiles:
      - flower
    ports:
      - "5555:5555"
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:5555/"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s

#############################
## SCYLLA 
  cassandra:
    image: cassandra:latest
    container_name: cassandra
    hostname: cassandra
    environment:
      CQLSH_HOST: cassandra 
      CQLSH_PORT: 9042 
      CQLVERSION: 3.4.6
      CASSANDRA_CLUSTER_NAME: datalab
      
    ports:
      - "9042:9042"
      - "17000:7000"
      - "10001:10000"
    volumes:
      - ./util:/util
      - ./cassandra/data/:/var/lib/cassandra

  cassandra-web:
    image: ipushc/cassandra-web
    container_name: cassandra-web
    depends_on:
      - cassandra
    ports:
      - 13000:80
    environment:
      CASSANDRA_HOST: cassandra
      CASSANDRA_PORT: 9042
      CASSANDRA_USER: cassandra
      CASSANDRA_PASSWORD: cassandra 
      HOST_PORT: ":80"  
#############################
## CLICKHOUSE
  clickhouse:
    image: clickhouse/clickhouse-server
    container_name: clickhouse
    hostname: clickhouse
    ports:
      - "18123:8123"
      - "29000:9000"
    environment:
      CLICKHOUSE_DB: "datalab"
      CLICKHOUSE_USER: "admin"
      CLICKHOUSE_PASSWORD: "admin"
      CLICKHOUSE_DEFAULT_ACCESS_MANAGEMENT: "1"
    volumes:
      - ./clickhouse/data:/var/lib/clickhouse/ 
      - ./util:/util

##############################
## MAGE
  mage:
    image: mageai/mageai:latest
    container_name: mage
    hostname: mage
    depends_on:
      - db
    command: mage start magic
    environment:
      ENV: dev
    ports:
      - 6789:6789
      - 33000:3000
    volumes:
      - ./mage:/home/src/
    
  


#############################
## GENERAL

configs:
  flags:
    file: ./airbyte/flags.yml

networks:
  datalab:
    driver: bridge
